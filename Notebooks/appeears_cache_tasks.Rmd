---
title: "appeears_cache_tasks"
author: "K. Enns"
date: "9/5/2018"
output: html_document
---

### Creates a dataframe out of all of the tasks submitted to AppEEARS under your account
```{r}
devtools::install_github("ennsk/AppEEARS4R")
library(jsonlite)
library(appeears)
library(httr)

username = ''
password = ''

token_response = appeears::appeears_start_session(username,password)
```

### Creates a dataframe out of all of the tasks submitted to AppEEARS under your account
```{r}
token         = paste("Bearer", token_response$token)
response      = GET("https://lpdaacsvc.cr.usgs.gov/appeears/api/task", add_headers(Authorization = token))
task_response = prettify(jsonlite::toJSON(content(response), auto_unbox = TRUE))
tasks = jsonlite::fromJSON(txt=task_response)
tasks
```

### Function to grab site name from task_name
```{r}
# name_length :  the length of most task names submitted.  kelloggswitchgrass_09_06_18_0839 would be name_length = 5
get_site_from_task = function(task_name_, name_length){
  elements = strsplit(task_name_, split = '_', fixed=TRUE)
  element_length = length(elements[[1]])
  if (element_length == name_length){
    site_name_ = elements[[1]][1]
    return (site_name_)
  }else if(element_length > name_length){
    num = element_length - name_length
    elem = elements[[1]][1]
    for (x in c(1:num)){
      elem = paste(elem, elements[[1]][x+1], sep='_')
    }
    return (elem)
  }else{
    sprintf('This task is missing information/invalid: %s', task_name_)
    return(FALSE)
  }
}

# Tests:
ele  = get_site_from_task('kelloggswitchgrass_09_06_18_0839', 5)
ele2 = get_site_from_task('NEON.D08.TALL.DP1.00042_TDs_v5', 3) 
ele3 = get_site_from_task('testsite_NDVI_v6',3)
# get_site_from_task('arsgacp1testest1452')
```

### Pullin in dataframe of phenocams
```{r}
c      = jsonlite::fromJSON('https://phenocam.sr.unh.edu/api/cameras/?format=json&limit=2000')
c = c$results
c_m=c$sitemetadata
c$sitemetadata=NULL
df =cbind(c, c_m)
df
```

### Grabbing 4 specific tasks (just for show)
```{r}
tasks[c(1,12,3,4),c(11,4,5)]
```

### Getting most recent task for each site that exists.  Errors on sites that haven't been submitted
###   as tasks yet
```{r}
# Just to clarify:  this script is meant to only grab the most recent task for each site submitted by the
#    cronjob script and create a df from that.  This script is a temporary solution to get around using
#    passwords to get the appropriate data for the application.
tasks_ndvi   = tasks[grep('_NDVI_v6', tasks$task_name), ]
task_sites   = tasks_ndvi$task_name

sites = c()
for (x in c(1:length(task_sites))){
  task = tasks_ndvi$task_name[x]
  site = strsplit(task, split = '_', fixed=TRUE)[[1]][1]
  if (site == 'HF'){
    site = paste0(site, '_Vivotek')
  }
  sites = c(sites, site)
}

headers = names(tasks_ndvi)

cache_df = data.frame(matrix(vector(), 0, 5,
                dimnames=list(c(), headers[c(4,5,7,10,11)])),
                stringsAsFactors=F)

for (x in c(1:length(sites))){
  print (sprintf('SITE: %s -----------------------------------', sites[x]))
  site_name   = sites[x]
  real_task_name = paste0(site_name, '_NDVI_v6')
  print(real_task_name)
  
  site_rows   = tasks_ndvi[grep(real_task_name, tasks_ndvi$task_name), ]
  
  add_row = site_rows[,c(4,5,7,10,11)]
  cache_df = rbind(cache_df, add_row)
}

n_occur_    = data.frame(table(cache_df$task_name))
duplicates_ = n_occur_[n_occur_$Freq > 1,]
duplicates_ = as.character(duplicates_$Var1)
duplicates_

cache_df
```

### Check unique tasks in Appeears based on phenocam sites to see is lengths match
```{r}
list = c()
for (i in cache_df$task_name){
  list = c(list, i)
}
print (length(unique(list)))
print (length(df$Sitename))
```

### Save out cache file for Most Current AppEEARS tasks
```{r}
saveRDS(cache_df, file = './www/cache_df_ndvi.df')
```

### Load in file
```{r}
dfdf = readRDS(file = './www/cache_df_ndvi.df')
```


### Function used to grab the row from cached appeears tasks using a phenocam site name
```{r}
get_appeears_task = function(name){
  task_pos = grep(site ,dfdf$task_name)
  
  task_pos
  
  for (i in c(1:length(task_pos))){
    row = get_site_from_task(dfdf[task_pos[i],]$task_name)
    if (row == site){
      task_ = dfdf[task_pos[i],]$task_name
    }
    print (get_site_from_task(dfdf[task_pos[i],]$task_name))
  }
  return (subset(dfdf, dfdf$task_name == task_))
}
site = 'arbutuslake'
get_appeears_task(site)

```


#### ---------
#### Extras :: Check for missing tasks, Bulk Delete
Qc tasks against phenocam api list to find missing tasks for appeears to re-submit
```{r}

NDVI_tasks   = tasks[grep('_NDVI_v6', tasks$task_name), ]
task_sites   = NDVI_tasks$task_name

sites = c()
for (x in c(1:length(task_sites))){
  task = NDVI_tasks$task_name[x]
  site = strsplit(task, split = '_', fixed=TRUE)[[1]][1]
  sites = c(sites, site)
}

cams_sites = unique(df$Sitename)
tasks_sites = unique(sites)

d = sites[duplicated(sites)]
d

length(sites)
length(tasks_sites)

length(cams_sites)
length(tasks_sites)

sites_to_download = setdiff(cams_sites, tasks_sites)
sites_to_download
length(sites_to_download)

n_occur    = data.frame(table(NDVI_tasks$task_name))
duplicates = n_occur[n_occur$Freq > 1,]
duplicates = as.character(duplicates$Var1)
duplicates
```


Bulk delete that was used when EVI tasks were accidently created named as NDVI layers.  
The first 316 were the incorrect names. 
```{r}
# # tasks[1:316,,]
# ids = tasks$task_id[1:100]
# task_names = tasks$task_name[1:100]
# 
# len_rows = length(tasks$task_id[1:100])
# 
# for (x in c(1:len_rows)){
#   print (ids[x])
#   token <- paste("Bearer", token_response$token)
#   task_id <- ids[x]
#   response <- DELETE(paste("https://lpdaacsvc.cr.usgs.gov/appeears/api/task/", task_id, sep = ""), add_headers(Authorization = token))
#   response$status_code
# }

```










